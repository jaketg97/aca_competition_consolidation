
@article{leslie_health_2018,
	title = {Health system measurement: {Harnessing} machine learning to advance global health},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Health system measurement},
	url = {https://dx.plos.org/10.1371/journal.pone.0204958},
	doi = {10.1371/journal.pone.0204958},
	language = {en},
	number = {10},
	urldate = {2021-02-18},
	journal = {PLOS ONE},
	author = {Leslie, Hannah H. and Zhou, Xin and Spiegelman, Donna and Kruk, Margaret E.},
	editor = {Uthman, Olalekan},
	month = oct,
	year = {2018},
	pages = {e0204958},
	file = {Leslie et al. - 2018 - Health system measurement Harnessing machine lear.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\872JCB7H\\Leslie et al. - 2018 - Health system measurement Harnessing machine lear.pdf:application/pdf},
}

@article{jayachandran_ve-question_nodate,
	title = {A ﬁve-question women’s agency index created using machine learning and qualitative interviews},
	abstract = {We develop a new short survey module for measuring women’s agency by combining mixed-methods data collection and machine learning. We select the best ﬁve survey questions for the module based on how strongly correlated they are with a “gold standard” measure of women’s agency. For a sample of 209 women in Haryana, India, we measure agency, ﬁrst, through a semi-structured in-depth interview and, second, through a large set of close-ended questions. The qualitative interviews provide rich data but are infeasible in most large-N studies. We use qualitative coding methods to score each woman’s agency based on the interview, which we treat as her “true” agency. To identify the subset of close-ended questions most predictive of the “truth,” we apply statistical methods similar to standard machine learning except that we specify how many survey questions are selected. The resulting 5-question index is as strongly correlated with the “truth” as is an index that uses all of the candidate questions. We also considered a second “gold standard” measure of agency, namely a real-stakes choice between money for oneself or one’s husband. This lab game, however, does not measure agency cleanly in our setting. Thus, our preferred survey measure of agency is the one validated against qualitative interviews.},
	language = {en},
	author = {Jayachandran, Seema and Biradavolu, Monica and Cooper, Jan},
	pages = {35},
	file = {Jayachandran et al. - A ﬁve-question women’s agency index created using .pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\J9LWJL7E\\Jayachandran et al. - A ﬁve-question women’s agency index created using .pdf:application/pdf},
}

@article{kshirsagar_household_2017,
	title = {Household poverty classification in data-scarce environments: a machine learning approach},
	shorttitle = {Household poverty classification in data-scarce environments},
	url = {http://arxiv.org/abs/1711.06813},
	abstract = {We describe a method to identify poor households in data-scarce countries by leveraging information contained in nationally representative household surveys. It employs standard statistical learning techniques—cross-validation and parameter regularization—which together reduce the extent to which the model is over-ﬁtted to match the idiosyncracies of observed survey data. The automated framework satisﬁes three important constraints of this development setting: i) The prediction model uses at most ten questions, which limits the costs of data collection; ii) No computation beyond simple arithmetic is needed to calculate the probability that a given household is poor, immediately after data on the ten indicators is collected; and iii) One speciﬁcation of the model (i.e. one scorecard) is used to predict poverty throughout a country that may be characterized by signiﬁcant sub-national differences. Using survey data from Zambia, the model’s out-ofsample predictions distinguish poor households from non-poor households using information contained in ten questions.},
	language = {en},
	urldate = {2021-02-18},
	journal = {arXiv:1711.06813 [stat]},
	author = {Kshirsagar, Varun and Wieczorek, Jerzy and Ramanathan, Sharada and Wells, Rachel},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06813},
	keywords = {Statistics - Applications, Statistics - Machine Learning},
	file = {Kshirsagar et al. - 2017 - Household poverty classification in data-scarce en.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\DVV88SKE\\Kshirsagar et al. - 2017 - Household poverty classification in data-scarce en.pdf:application/pdf},
}

@article{nan_feature-budgeted_nodate,
	title = {Feature-{Budgeted} {Random} {Forest}},
	abstract = {We seek decision rules for prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-speciﬁed average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which ampliﬁes costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax costweighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate competitive accuracy-cost curves against state-of-the-art prediction-time algorithms.},
	language = {en},
	author = {Nan, Feng and Wang, Joseph and Saligrama, Venkatesh},
	pages = {9},
	file = {Nan et al. - Feature-Budgeted Random Forest.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\PIZGXR5Z\\Nan et al. - Feature-Budgeted Random Forest.pdf:application/pdf},
}

@misc{cerulli_sctree_2020,
	title = {{SCTREE}: {Stata} module to implement classification trees via optimal pruning, bagging, random forests, and boosting methods},
	shorttitle = {{SCTREE}},
	url = {https://ideas.repec.org/c/boc/bocode/s458645.html},
	abstract = {sctree is a Stata wrapper for the R functions "tree()", "randomForest()", and "gbm()". It allows to implement the following classification tree models: (1) classification tree with optimal pruning, (2) bagging, (3) random forests, and (4) boosting.},
	urldate = {2021-02-18},
	publisher = {Boston College Department of Economics},
	author = {Cerulli, Giovanni},
	month = may,
	year = {2020},
	note = {Language: en
Publication Title: Statistical Software Components},
	keywords = {bagging, boosting, classification trees, pruning, random forests, Stata},
	file = {Snapshot:C\:\\Users\\jtg3519\\Zotero\\storage\\ENVZGS7T\\s458645.html:text/html},
}

@article{strobl_bias_2007,
	title = {Bias in random forest variable importance measures: {Illustrations}, sources and a solution},
	volume = {8},
	issn = {1471-2105},
	shorttitle = {Bias in random forest variable importance measures},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25},
	doi = {10.1186/1471-2105-8-25},
	abstract = {Background: Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.
Results: Simulation studies are presented illustrating that, when random forest variable importance measures are used with data of varying types, the results are misleading because suboptimal predictor variables may be artificially preferred in variable selection. The two mechanisms underlying this deficiency are biased variable selection in the individual classification trees used to build the random forest on one hand, and effects induced by bootstrap sampling with replacement on the other hand.
Conclusion: We propose to employ an alternative implementation of random forests, that provides unbiased variable selection in the individual classification trees. When this method is applied using subsampling without replacement, the resulting variable importance measures can be used reliably for variable selection even in situations where the potential predictor variables vary in their scale of measurement or their number of categories. The usage of both random forest algorithms and their variable importance measures in the R system for statistical computing is illustrated and documented thoroughly in an application re-analyzing data from a study on RNA editing. Therefore the suggested method can be applied straightforwardly by scientists in bioinformatics research.},
	language = {en},
	number = {1},
	urldate = {2021-02-18},
	journal = {BMC Bioinformatics},
	author = {Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten},
	month = dec,
	year = {2007},
	pages = {25},
	file = {Strobl et al. - 2007 - Bias in random forest variable importance measures.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\PJ4Q2ET3\\Strobl et al. - 2007 - Bias in random forest variable importance measures.pdf:application/pdf},
}

@article{schonlau_random_2020,
	title = {The random forest algorithm for statistical learning},
	volume = {20},
	issn = {1536-867X, 1536-8734},
	url = {http://journals.sagepub.com/doi/10.1177/1536867X20909688},
	doi = {10.1177/1536867X20909688},
	abstract = {Random forests (Breiman, 2001, Machine Learning 45: 5–32) is a statistical- or machine-learning algorithm for prediction. In this article, we introduce a corresponding new command, rforest. We overview the random forest algorithm and illustrate its use with two examples: The ﬁrst example is a classiﬁcation problem that predicts whether a credit card holder will default on his or her debt. The second example is a regression problem that predicts the logscaled number of shares of online news articles. We conclude with a discussion that summarizes key points demonstrated in the examples.},
	language = {en},
	number = {1},
	urldate = {2021-02-18},
	journal = {The Stata Journal: Promoting communications on statistics and Stata},
	author = {Schonlau, Matthias and Zou, Rosie Yuyan},
	month = mar,
	year = {2020},
	pages = {3--29},
	file = {Schonlau and Zou - 2020 - The random forest algorithm for statistical learni.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\ZU8E9RTW\\Schonlau and Zou - 2020 - The random forest algorithm for statistical learni.pdf:application/pdf},
}

@article{das_cost_2020,
	title = {Cost {Aware} {Feature} {Elicitation}},
	abstract = {Motivated by clinical tasks where acquiring certain features such as FMRI or blood tests can be expensive, we address the problem of test-time elicitation of features. We formulate the problem of costaware feature elicitation as an optimization problem with trade-off between performance and feature acquisition cost. Our experiments on three real-world medical tasks demonstrate the efficacy and effectiveness of our proposed approach in minimizing costs and maximizing performance.},
	language = {en},
	journal = {San Diego},
	author = {Das, Srijita and Iyer, Rishabh and Natarajan, Sriraam},
	year = {2020},
	pages = {6},
	file = {Das et al. - 2020 - Cost Aware Feature Elicitation.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\DKRU4GYE\\Das et al. - 2020 - Cost Aware Feature Elicitation.pdf:application/pdf},
}

@misc{parr_beware_nodate,
	title = {Beware {Default} {Random} {Forest} {Importances}},
	url = {http://explained.ai/decision-tree-viz/index.html},
	abstract = {Training a model that accurately predicts outcomes is great, but most of the time you don't just need predictions, you want to be able to interpret your model. The problem is that the scikit-learn Random Forest feature importance and R's default Random Forest feature importance strategies are biased. To get reliable results in Python, use permutation importance, provided here and in our rfpimp package (via pip). For R, use importance=T in the Random Forest constructor then type=1 in R's importance() function.},
	urldate = {2021-02-19},
	author = {Parr, Terrence and Turgutlu, Kerem},
	file = {Parr and Turgutlu - Beware Default Random Forest Importances.html:C\:\\Users\\jtg3519\\Zotero\\storage\\8DS3AZW2\\Parr and Turgutlu - Beware Default Random Forest Importances.html:text/html},
}

@article{li_feature_2018,
	title = {Feature {Selection}: {A} {Data} {Perspective}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Feature {Selection}},
	url = {https://dl.acm.org/doi/10.1145/3136625},
	doi = {10.1145/3136625},
	language = {en},
	number = {6},
	urldate = {2021-02-19},
	journal = {ACM Computing Surveys},
	author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
	month = jan,
	year = {2018},
	pages = {1--45},
	file = {Li et al. - 2018 - Feature Selection A Data Perspective.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\4T5NB79P\\Li et al. - 2018 - Feature Selection A Data Perspective.pdf:application/pdf},
}

@article{janitza_computationally_2018,
	title = {A computationally fast variable importance test for random forests for high-dimensional data},
	volume = {12},
	issn = {1862-5347, 1862-5355},
	url = {http://link.springer.com/10.1007/s11634-016-0276-4},
	doi = {10.1007/s11634-016-0276-4},
	abstract = {Random forests are a commonly used tool for classiﬁcation and for ranking candidate predictors based on the so-called variable importance measures. These measures attribute scores to the variables reﬂecting their importance. A drawback of variable importance measures is that there is no natural cutoff that can be used to discriminate between important and non-important variables. Several approaches, for example approaches based on hypothesis testing, were developed for addressing this problem. The existing testing approaches require the repeated computation of random forests. While for low-dimensional settings those approaches might be computationally tractable, for high-dimensional settings typically including thousands of candidate predictors, computing time is enormous. In this article a computationally fast heuristic variable importance test is proposed that is appropriate for high-dimensional data where many variables do not carry any information. The testing approach is based on a modiﬁed version of the permutation variable importance, which is inspired by crossvalidation procedures. The new approach is tested and compared to the approach of Altmann and colleagues using simulation studies, which are based on real data from high-dimensional binary classiﬁcation settings. The new approach controls the type I error and has at least comparable power at a substantially smaller computation time in the studies. Thus, it might be used as a computationally fast alternative to existing procedures for high-dimensional data settings where many variables do not carry any information. The new approach is implemented in the R package vita.},
	language = {en},
	number = {4},
	urldate = {2021-02-20},
	journal = {Advances in Data Analysis and Classification},
	author = {Janitza, Silke and Celik, Ender and Boulesteix, Anne-Laure},
	month = dec,
	year = {2018},
	pages = {885--915},
	file = {Janitza2018_Article_AComputationallyFastVariableIm (1).pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\GMX5RY9I\\Janitza2018_Article_AComputationallyFastVariableIm (1).pdf:application/pdf},
}

@article{hapfelmeier_new_2013,
	title = {A new variable selection approach using {Random} {Forests}},
	volume = {60},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947312003490},
	doi = {10.1016/j.csda.2012.09.020},
	abstract = {Random Forests are frequently applied as they achieve a high prediction accuracy and have the ability to identify informative variables. Several approaches for variable selection have been proposed to combine and intensify these qualities. An extensive review of the corresponding literature led to the development of a new approach that is based on the theoretical framework of permutation tests and meets important statistical properties. A comparison to another eight popular variable selection methods in three simulation studies and four real data applications indicated that: the new approach can also be used to control the test-wise and family-wise error rate, provides a higher power to distinguish relevant from irrelevant variables and leads to models which are located among the very best performing ones. In addition, it is equally applicable to regression and classification problems.},
	language = {en},
	urldate = {2021-02-20},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hapfelmeier, A. and Ulm, K.},
	month = apr,
	year = {2013},
	pages = {50--69},
	file = {Hapfelmeier and Ulm - 2013 - A new variable selection approach using Random For.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\BEE375YU\\Hapfelmeier and Ulm - 2013 - A new variable selection approach using Random For.pdf:application/pdf},
}

@article{altmann_permutation_2010,
	title = {Permutation importance: a corrected feature importance measure},
	volume = {26},
	issn = {1460-2059, 1367-4803},
	shorttitle = {Permutation importance},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btq134},
	doi = {10.1093/bioinformatics/btq134},
	abstract = {Motivation: In life sciences, interpretability of machine learning models is as important as their prediction accuracy. Linear models are probably the most frequently used methods for assessing feature relevance, despite their relative inﬂexibility. However, in the past years effective estimators of feature relevance have been derived for highly complex or non-parametric models such as support vector machines and RandomForest (RF) models. Recently, it has been observed that RF models are biased in such a way that categorical variables with a large number of categories are preferred.},
	language = {en},
	number = {10},
	urldate = {2021-02-20},
	journal = {Bioinformatics},
	author = {Altmann, André and Toloşi, Laura and Sander, Oliver and Lengauer, Thomas},
	month = may,
	year = {2010},
	pages = {1340--1347},
	file = {Altmann et al. - 2010 - Permutation importance a corrected feature import.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\3V78FL9R\\Altmann et al. - 2010 - Permutation importance a corrected feature import.pdf:application/pdf},
}
