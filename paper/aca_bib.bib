
@article{belloni_high-dimensional_nodate,
	title = {High-{Dimensional} {Methods} and {Inference} on {Structural} and {Treatment} {Effects}},
	language = {en},
	author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
	pages = {28},
	file = {Belloni et al. - High-Dimensional Methods and Inference on Structur.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\TYVH5MI2\\Belloni et al. - High-Dimensional Methods and Inference on Structur.pdf:application/pdf},
}

@article{hes_randomization_2017,
	title = {Randomization {Inference} with {Stata}: {A} {Guide} and {Software}},
	volume = {17},
	issn = {1536-867X, 1536-8734},
	shorttitle = {Randomization {Inference} with {Stata}},
	url = {http://journals.sagepub.com/doi/10.1177/1536867X1701700306},
	doi = {10.1177/1536867X1701700306},
	abstract = {Randomization inference or permutation tests are only sporadically used in economics and other social sciences—this despite a steep increase in randomization in field and laboratory experiments that provide perfect experimental setups for applying randomization inference. In the context of causal inference, such tests can handle problems often faced by applied researchers, including issues arising in the context of small samples, stratified or clustered treatment assignments, or nonstandard randomization techniques. Standard statistical software packages have either no implementation of randomization tests or very basic implementations. Whenever researchers use randomization inference, they regularly code individual program routines, risking inconsistencies and coding mistakes. In this article, I show how randomization inference can best be conducted in Stata and introduce a new command, ritest, to simplify such analyses. I illustrate this approach's usefulness by replicating the results in Fujiwara and Wantchekon (2013, American Economic Journal: Applied Economics 5: 241–255) and running simulations. The applications cover clustered and stratified assignments, with varying cluster sizes, pairwise randomization, and the computation of nonapproximate p-values. The applications also touch upon joint hypothesis testing with randomization inference.},
	language = {en},
	number = {3},
	urldate = {2021-02-18},
	journal = {The Stata Journal: Promoting communications on statistics and Stata},
	author = {Heß, Simon},
	month = sep,
	year = {2017},
	pages = {630--651},
	file = {Heß - 2017 - Randomization Inference with Stata A Guide and So.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\QHPNS257\\Heß - 2017 - Randomization Inference with Stata A Guide and So.pdf:application/pdf},
}

@article{belloni_inference_2014,
	title = {Inference on {Treatment} {Effects} after {Selection} among {High}-{Dimensional} {Controls}},
	volume = {81},
	issn = {0034-6527, 1467-937X},
	url = {https://academic.oup.com/restud/article-lookup/doi/10.1093/restud/rdt044},
	doi = {10.1093/restud/rdt044},
	abstract = {We propose robust methods for inference about the effect of a treatment variable on a scalar outcome in the presence of very many regressors in a model with possibly non-Gaussian and heteroscedastic disturbances. We allow for the number of regressors to be larger than the sample size. To make informative inference feasible, we require the model to be approximately sparse; that is, we require that the effect of confounding factors can be controlled for up to a small approximation error by including a relatively small number of variables whose identities are unknown. The latter condition makes it possible to estimate the treatment effect by selecting approximately the right set of regressors. We develop a novel estimation and uniformly valid inference method for the treatment effect in this setting, called the “post-double-selection” method. The main attractive feature of our method is that it allows for imperfect selection of the controls and provides conﬁdence intervals that are valid uniformly across a large class of models. In contrast, standard post-model selection estimators fail to provide uniform inference even in simple cases with a small, ﬁxed number of controls. Thus, our method resolves the problem of uniform inference after model selection for a large, interesting class of models. We also present a generalization of our method to a fully heterogeneous model with a binary treatment variable. We illustrate the use of the developed methods with numerical simulations and an application that considers the effect of abortion on crime rates.},
	language = {en},
	number = {2},
	urldate = {2021-02-18},
	journal = {The Review of Economic Studies},
	author = {Belloni, A. and Chernozhukov, V. and Hansen, C.},
	month = apr,
	year = {2014},
	pages = {608--650},
	file = {Belloni et al. - 2014 - Inference on Treatment Effects after Selection amo.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\JYHIVL47\\Belloni et al. - 2014 - Inference on Treatment Effects after Selection amo.pdf:application/pdf},
}

@book{clarke_principles_2009,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Principles and {Theory} for {Data} {Mining} and {Machine} {Learning}},
	isbn = {978-0-387-98134-5 978-0-387-98135-2},
	url = {http://link.springer.com/10.1007/978-0-387-98135-2},
	language = {en},
	urldate = {2021-02-18},
	publisher = {Springer New York},
	author = {Clarke, Bertrand and Fokoue, Ernest and Zhang, Hao Helen},
	year = {2009},
	doi = {10.1007/978-0-387-98135-2},
	file = {Clarke et al. - 2009 - Principles and Theory for Data Mining and Machine .pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\EXCCESRC\\Clarke et al. - 2009 - Principles and Theory for Data Mining and Machine .pdf:application/pdf},
}

@article{athey_machine_2019,
	title = {Machine {Learning} {Methods} {That} {Economists} {Should} {Know} {About}},
	volume = {11},
	issn = {1941-1383, 1941-1391},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-economics-080217-053433},
	doi = {10.1146/annurev-economics-080217-053433},
	abstract = {We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.},
	language = {en},
	number = {1},
	urldate = {2021-02-18},
	journal = {Annual Review of Economics},
	author = {Athey, Susan and Imbens, Guido W.},
	month = aug,
	year = {2019},
	pages = {685--725},
	file = {Athey and Imbens - 2019 - Machine Learning Methods That Economists Should Kn.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\K7UCMVZ6\\Athey and Imbens - 2019 - Machine Learning Methods That Economists Should Kn.pdf:application/pdf},
}

@article{dean_attending_nodate,
	title = {Attending kindergarten improves cognitive development in {India}, but all kindergartens are not equal},
	abstract = {Early childhood is a critical period for child development, and several studies ﬁnd high returns to formal early schooling (e.g., pre-K) in developed countries. However, there is limited evidence on whether formal pre-primary schooling is an eﬀective model in developing countries. We study the impacts of attending kindergarten on child development in Karnataka, India, through a randomized evaluation. We partnered with a private kindergarten provider to oﬀer two-year scholarships to children in low-income families. Children who attend the partner kindergarten due to the scholarship experience a 0.8 standard deviation gain in cognitive development. Some children induced to attend the partner kindergarten would not have attended kindergarten, while others would have attended a diﬀerent kindergarten. We use machine learning techniques to predict each child’s counterfactual activity and then estimate separate treatment effects for each type of switcher. We ﬁnd that the short-run eﬀect on cognition is driven mostly by children who would have otherwise not attended kindergarten. About 40\% of the eﬀect on cognitive development persists through ﬁrst grade, with more persistence for higher-order thinking skills. In contrast, we ﬁnd no eﬀects on socioemotional development, which could be due to most children interacting with other children in daycare centers even if they do not attend kindergarten.},
	language = {en},
	author = {Dean, Joshua T and Jayachandran, Seema},
	pages = {64},
	file = {Dean and Jayachandran - Attending kindergarten improves cognitive developm.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\9G8DYSV8\\Dean and Jayachandran - Attending kindergarten improves cognitive developm.pdf:application/pdf},
}

@article{young_channeling_2019,
	title = {Channeling {Fisher}: {Randomization} {Tests} and the {Statistical} {Insignificance} of {Seemingly} {Significant} {Experimental} {Results}*},
	volume = {134},
	issn = {0033-5533, 1531-4650},
	shorttitle = {Channeling {Fisher}},
	url = {https://academic.oup.com/qje/article/134/2/557/5195544},
	doi = {10.1093/qje/qjy029},
	language = {en},
	number = {2},
	urldate = {2021-02-18},
	journal = {The Quarterly Journal of Economics},
	author = {Young, Alwyn},
	month = may,
	year = {2019},
	pages = {557--598},
	file = {Young - 2019 - Channeling Fisher Randomization Tests and the Sta.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\58PWB878\\Young - 2019 - Channeling Fisher Randomization Tests and the Sta.pdf:application/pdf},
}

@article{leslie_health_2018,
	title = {Health system measurement: {Harnessing} machine learning to advance global health},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Health system measurement},
	url = {https://dx.plos.org/10.1371/journal.pone.0204958},
	doi = {10.1371/journal.pone.0204958},
	language = {en},
	number = {10},
	urldate = {2021-02-18},
	journal = {PLOS ONE},
	author = {Leslie, Hannah H. and Zhou, Xin and Spiegelman, Donna and Kruk, Margaret E.},
	editor = {Uthman, Olalekan},
	month = oct,
	year = {2018},
	pages = {e0204958},
	file = {Leslie et al. - 2018 - Health system measurement Harnessing machine lear.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\872JCB7H\\Leslie et al. - 2018 - Health system measurement Harnessing machine lear.pdf:application/pdf},
}

@article{jayachandran_ve-question_nodate,
	title = {A ﬁve-question women’s agency index created using machine learning and qualitative interviews},
	abstract = {We develop a new short survey module for measuring women’s agency by combining mixed-methods data collection and machine learning. We select the best ﬁve survey questions for the module based on how strongly correlated they are with a “gold standard” measure of women’s agency. For a sample of 209 women in Haryana, India, we measure agency, ﬁrst, through a semi-structured in-depth interview and, second, through a large set of close-ended questions. The qualitative interviews provide rich data but are infeasible in most large-N studies. We use qualitative coding methods to score each woman’s agency based on the interview, which we treat as her “true” agency. To identify the subset of close-ended questions most predictive of the “truth,” we apply statistical methods similar to standard machine learning except that we specify how many survey questions are selected. The resulting 5-question index is as strongly correlated with the “truth” as is an index that uses all of the candidate questions. We also considered a second “gold standard” measure of agency, namely a real-stakes choice between money for oneself or one’s husband. This lab game, however, does not measure agency cleanly in our setting. Thus, our preferred survey measure of agency is the one validated against qualitative interviews.},
	language = {en},
	author = {Jayachandran, Seema and Biradavolu, Monica and Cooper, Jan},
	pages = {35},
	file = {Jayachandran et al. - A ﬁve-question women’s agency index created using .pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\J9LWJL7E\\Jayachandran et al. - A ﬁve-question women’s agency index created using .pdf:application/pdf},
}

@article{kshirsagar_household_2017,
	title = {Household poverty classification in data-scarce environments: a machine learning approach},
	shorttitle = {Household poverty classification in data-scarce environments},
	url = {http://arxiv.org/abs/1711.06813},
	abstract = {We describe a method to identify poor households in data-scarce countries by leveraging information contained in nationally representative household surveys. It employs standard statistical learning techniques—cross-validation and parameter regularization—which together reduce the extent to which the model is over-ﬁtted to match the idiosyncracies of observed survey data. The automated framework satisﬁes three important constraints of this development setting: i) The prediction model uses at most ten questions, which limits the costs of data collection; ii) No computation beyond simple arithmetic is needed to calculate the probability that a given household is poor, immediately after data on the ten indicators is collected; and iii) One speciﬁcation of the model (i.e. one scorecard) is used to predict poverty throughout a country that may be characterized by signiﬁcant sub-national differences. Using survey data from Zambia, the model’s out-ofsample predictions distinguish poor households from non-poor households using information contained in ten questions.},
	language = {en},
	urldate = {2021-02-18},
	journal = {arXiv:1711.06813 [stat]},
	author = {Kshirsagar, Varun and Wieczorek, Jerzy and Ramanathan, Sharada and Wells, Rachel},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06813},
	keywords = {Statistics - Applications, Statistics - Machine Learning},
	file = {Kshirsagar et al. - 2017 - Household poverty classification in data-scarce en.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\DVV88SKE\\Kshirsagar et al. - 2017 - Household poverty classification in data-scarce en.pdf:application/pdf},
}

@article{lc_appendix_nodate,
	title = {Appendix {Table} 1: {Descriptive} {Statistics} for {Variables} {Used} in {Analysis}},
	language = {en},
	journal = {Health Aff},
	author = {Lc, Baker and Mk, Bundorf},
	pages = {3},
	file = {Lc and Mk - Appendix Table 1 Descriptive Statistics for Varia.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\AEQ4ZPNW\\Lc and Mk - Appendix Table 1 Descriptive Statistics for Varia.pdf:application/pdf},
}

@article{scheffler_differing_2016,
	title = {Differing {Impacts} {Of} {Market} {Concentration} {On} {Affordable} {Care} {Act} {Marketplace} {Premiums}},
	volume = {35},
	issn = {0278-2715, 1544-5208},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2015.1229},
	doi = {10.1377/hlthaff.2015.1229},
	abstract = {Recent increases in market concentration among health plans, hospitals, and medical groups raise questions about what impact such mergers are having on costs to consumers. We examined the impact of market concentration on the growth of health insurance premiums between 2014 and 2015 in two Affordable Care Act state-based Marketplaces: Covered California and NY State of Health. We measured health plan, hospital, and medical group market concentration using the well-known Herfindahl-Hirschman Index (HHI) and used a multivariate regression model to relate these measures to premium growth. Both states exhibited a positive association between hospital concentration and premium growth and a positive (but not statistically significant) association between medical group concentration and premium growth. Our results for health plan concentration differed between the two states: It was positively associated with premium growth in New York but negatively associated with premium growth in California. The health plan concentration finding in Covered California may be the result of its selectively contracting with health plans.},
	language = {en},
	number = {5},
	urldate = {2021-02-18},
	journal = {Health Affairs},
	author = {Scheffler, Richard M. and Arnold, Daniel R. and Fulton, Brent D. and Glied, Sherry A.},
	month = may,
	year = {2016},
	pages = {880--888},
	file = {Scheffler et al. - 2016 - Differing Impacts Of Market Concentration On Affor.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\AC9R7UFL\\Scheffler et al. - 2016 - Differing Impacts Of Market Concentration On Affor.pdf:application/pdf},
}

@article{melnick_increased_2011,
	title = {The {Increased} {Concentration} {Of} {Health} {Plan} {Markets} {Can} {Benefit} {Consumers} {Through} {Lower} {Hospital} {Prices}},
	volume = {30},
	issn = {0278-2715, 1544-5208},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2010.0406},
	doi = {10.1377/hlthaff.2010.0406},
	abstract = {The long-term trend of consolidation among US health plans has raised providers’ concerns that the concentration of health plan markets can depress their prices. Although our study confirmed that, it also revealed a more complex picture. First, we found that 64 percent of hospitals operate in markets where health plans are not very concentrated, and only 7 percent are in markets that are dominated by a few health plans. Second, we found that in most markets, hospital market concentration exceeds health plan concentration. Third, our study confirmed earlier studies showing that greater hospital market concentration leads to higher hospital prices. Fourth, we found that hospital prices in the most concentrated health plan markets are approximately 12 percent lower than in more competitive health plan markets. Overall, our results show that more concentrated health plan markets can counteract the price-increasing effects of concentrated hospital markets, and that—contrary to conventional wisdom—increased health plan concentration benefits consumers through lower hospital prices as long as health plan markets remain competitive. Our findings also suggest that consumers would benefit from policies that maintained competition in hospital markets or that would restore competition to hospital markets that are uncompetitive.},
	language = {en},
	number = {9},
	urldate = {2021-02-18},
	journal = {Health Affairs},
	author = {Melnick, Glenn A. and Shen, Yu-Chu and Wu, Vivian Yaling},
	month = sep,
	year = {2011},
	pages = {1728--1733},
	file = {Melnick et al. - 2011 - The Increased Concentration Of Health Plan Markets.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\H4VISQVS\\Melnick et al. - 2011 - The Increased Concentration Of Health Plan Markets.pdf:application/pdf},
}

@article{capps_hospital_2004,
	title = {Hospital {Consolidation} {And} {Negotiated} {PPO} {Prices}},
	volume = {23},
	issn = {0278-2715, 1544-5208},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.23.2.175},
	doi = {10.1377/hlthaff.23.2.175},
	abstract = {We examine the effects of hospital consolidation on the actual prices paid by preferred provider organizations. We find that price increases following consolidations among nearby hospitals invariably equaled or exceeded median price increases among other hospitals in the same market. Using multivariate regression analysis, we find that consolidation enables hospitals to increase prices in three of the four markets studied; these increases are generally statistically significant. In the remaining market, the measured effect was zero. Our results suggest that some, but not all, consolidations of competing hospitals facilitate price increases. We conclude that antitrust scrutiny of hospital consolidation is warranted.},
	language = {en},
	number = {2},
	urldate = {2021-02-18},
	journal = {Health Affairs},
	author = {Capps, Cory and Dranove, David},
	month = mar,
	year = {2004},
	pages = {175--181},
	file = {Capps and Dranove - 2004 - Hospital Consolidation And Negotiated PPO Prices.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\XU5B5FCX\\Capps and Dranove - 2004 - Hospital Consolidation And Negotiated PPO Prices.pdf:application/pdf},
}

@article{gresenz_updated_2004,
	title = {Updated {Variable}-{Radius} {Measures} of {Hospital} {Competition}},
	language = {en},
	journal = {Health Services Research},
	author = {Gresenz, Carole Roan and Rogowski, Jeannette and Escarce, JoseJ},
	year = {2004},
	pages = {14},
	file = {Gresenz et al. - 2004 - Updated Variable-Radius Measures of Hospital Compe.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\6A6DSW56\\Gresenz et al. - 2004 - Updated Variable-Radius Measures of Hospital Compe.pdf:application/pdf},
}

@article{abraham_rural-urban_2016,
	title = {Rural-{Urban} {Differences} in {Insurer} {Participation} for {Marketplace}-{Based} {Coverage}},
	language = {en},
	author = {Abraham, Jean and Drake, Coleman and McCullough, Jeffrey S and Simon, Kosali},
	year = {2016},
	pages = {5},
	file = {Abraham et al. - 2016 - Rural-Urban Differences in Insurer Participation f.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\TSCUJ8MY\\Abraham et al. - 2016 - Rural-Urban Differences in Insurer Participation f.pdf:application/pdf},
}

@article{morrisey_five-state_2017,
	title = {Five-state study of {ACA} marketplace competition},
	language = {en},
	journal = {The Brookings Institution},
	author = {Morrisey, Michael A and Rivlin, Alice M and Nathan, Richard P and Hall, Mark A},
	month = feb,
	year = {2017},
	pages = {22},
	file = {Morrisey et al. - FIVE-STATE STUDY OF ACA MARKETPLACE COMPETITION.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\TZYIW476\\Morrisey et al. - FIVE-STATE STUDY OF ACA MARKETPLACE COMPETITION.pdf:application/pdf},
}

@article{polyakova_aca_nodate,
	title = {{ACA} {Marketplace} {Premiums} and {Competition} {Among} {Hospitals} and {Physician} {Practices}},
	language = {en},
	author = {Polyakova, Maria and Bundorf, M Kate and Kessler, Daniel P and Baker, Laurence C},
	pages = {6},
	file = {Polyakova et al. - ACA Marketplace Premiums and Competition Among Hos.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\B6Z9BQFG\\Polyakova et al. - ACA Marketplace Premiums and Competition Among Hos.pdf:application/pdf},
}

@article{berenson_addressing_2015,
	title = {Addressing {Pricing} {Power} in {Integrated} {Delivery}: {The} {Limits} of {Antitrust}},
	volume = {40},
	issn = {0361-6878, 1527-1927},
	shorttitle = {Addressing {Pricing} {Power} in {Integrated} {Delivery}},
	url = {https://read.dukeupress.edu/jhppl/article/40/4/711-744/13749},
	doi = {10.1215/03616878-3150026},
	abstract = {Prices are the major driver of why the United States spends so much more on health care than other countries do. The pricing power that hospitals have garnered recently has resulted from consolidated delivery systems and concentrated markets, leading to enhanced negotiating leverage. But consolidation may be the wrong frame for viewing the problem of high and highly variable prices; many “must-have” hospitals achieve their pricing power from sources other than consolidation, for example, reputation. Further, the frame of consolidation leads to unrealistic expectations for what antitrust’s role in addressing pricing power should be, especially because in the wake of two periods of merger “manias” and “frenzies” many markets already lack effective competition. It is particularly challenging for antitrust to address extant monopolies lawfully attained. New payment and delivery models being pioneered in Medicare, especially those built around accountable care organizations (ACOs), offer an opportunity to reduce pricing power, but only if they are implemented with a clear eye on the impact on prices in commercial insurance markets. This article proposes approaches that public and private payers should consider to complement the role of antitrust to assure that ACOs will actually help control costs in commercial markets as well as in Medicare and Medicaid.},
	language = {en},
	number = {4},
	urldate = {2021-02-18},
	journal = {Journal of Health Politics, Policy and Law},
	author = {Berenson, R.},
	month = jan,
	year = {2015},
	pages = {711--744},
	file = {Berenson - 2015 - Addressing Pricing Power in Integrated Delivery T.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\HB2EEGYN\\Berenson - 2015 - Addressing Pricing Power in Integrated Delivery T.pdf:application/pdf},
}

@article{boozary_association_2019,
	title = {The {Association} {Between} {Hospital} {Concentration} {And} {Insurance} {Premiums} {In} {ACA} {Marketplaces}},
	volume = {38},
	issn = {0278-2715, 1544-5208},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2018.05491},
	doi = {10.1377/hlthaff.2018.05491},
	abstract = {Keeping the Affordable Care Act’s health insurance Marketplaces financially accessible is critically important to their viability. While the relationship between the number of insurers and Marketplace premiums has received widespread attention, the role of hospital market concentration on premiums has been understudied. We examined the relationship between hospital market concentration and Marketplace insurance premiums in the period 2014–17, the extent to which the number of insurers modified this relationship, and whether community-level characteristics were associated with varying levels of concentration. We found that areas with the highest levels of hospital market concentration had annual premiums that were, on average, 5 percent higher than those in the least concentrated areas. Additionally, while an increased number of insurers was independently associated with lower premiums, that was not sufficient to offset the effects of increased hospital concentration on premium costs. Communities with lower socioeconomic status (as measured by median income) were more likely to have higher hospital market concentration. However, this was not consistent across all measures of socioeconomic status, such as measures of unemployment, use of the Supplemental Nutrition Assistance Program, and education. These findings help underscore the importance of exploring antitrust policy and other efforts that may reduce hospital concentration and help keep Marketplace premiums affordable.},
	language = {en},
	number = {4},
	urldate = {2021-02-18},
	journal = {Health Affairs},
	author = {Boozary, Andrew S. and Feyman, Yevgeniy and Reinhardt, Uwe E. and Jha, Ashish K.},
	month = apr,
	year = {2019},
	pages = {668--674},
	file = {Boozary et al. - 2019 - The Association Between Hospital Concentration And.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\M27E46GP\\Boozary et al. - 2019 - The Association Between Hospital Concentration And.pdf:application/pdf},
}

@article{fulton_health_2017,
	title = {Health {Care} {Market} {Concentration} {Trends} {In} {The} {United} {States}: {Evidence} {And} {Policy} {Responses}},
	volume = {36},
	issn = {0278-2715, 1544-5208},
	shorttitle = {Health {Care} {Market} {Concentration} {Trends} {In} {The} {United} {States}},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2017.0556},
	doi = {10.1377/hlthaff.2017.0556},
	abstract = {Policy makers and analysts have been voicing concerns about the increasing concentration of health care providers and health insurers in markets nationwide, including the potential adverse effect on the cost and quality of health care. The Council of Economic Advisers recently expressed its concern about the lack of estimates of market concentration in many sectors of the US economy. To address this gap in health care, this study analyzed market concentration trends in the United States from 2010 to 2016 for hospitals, physician organizations, and health insurers. Hospital and physician organization markets became increasingly concentrated over this time period. Concentration among primary care physicians increased the most, partially because hospitals and health care systems acquired primary care physician organizations. In 2016, 90 percent of Metropolitan Statistical Areas (MSAs) were highly concentrated for hospitals, 65 percent for specialist physicians, 39 percent for primary care physicians, and 57 percent for insurers. Ninety-one percent of the 346 MSAs analyzed may have warranted concern and scrutiny because of their concentration levels in 2016 and changes in their concentrations since 2010. Public policies that enhance competition are needed, such as stricter enforcement of antitrust laws, reducing barriers to entry, and restricting anticompetitive behaviors.},
	language = {en},
	number = {9},
	urldate = {2021-02-18},
	journal = {Health Affairs},
	author = {Fulton, Brent D.},
	month = sep,
	year = {2017},
	pages = {1530--1538},
	file = {Fulton - 2017 - Health Care Market Concentration Trends In The Uni.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\EHWPMQ84\\Fulton - 2017 - Health Care Market Concentration Trends In The Uni.pdf:application/pdf},
}

@article{fang_why_2020,
	title = {Why {Do} {Insurers} {Selectively} {Enter} {Counties} within a {Rating} {Area} on the {ACA} {Marketplaces}?},
	abstract = {Each state has a set number of geographic “rating areas,” typically made up of counties, that all insurers participating in the state’s ACA Marketplace must uniformly use as the geographic unit for varying premiums. This paper shows that it is quite common for insurers to not sell any plans in certain counties, while serving other counties in the same rating area. We provide empirical evidence that risk screening and provider network setup costs are the main mechanisms driving these selective entry patterns. We ﬁnd no evidence that insurers limit their service area to avoid competition.},
	language = {en},
	author = {Fang, Hanming and Ko, Ami},
	year = {2020},
	pages = {51},
	file = {Fang and Ko - Why Do Insurers Selectively Enter Counties within .pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\XG3JC875\\Fang and Ko - Why Do Insurers Selectively Enter Counties within .pdf:application/pdf},
}

@article{gaynor_making_2017,
	title = {Making {Health} {Care} {Markets} {Work}: {Competition} {Policy} for {Health} {Care}},
	issn = {1556-5068},
	shorttitle = {Making {Health} {Care} {Markets} {Work}},
	url = {http://www.ssrn.com/abstract=2964912},
	doi = {10.2139/ssrn.2964912},
	language = {en},
	urldate = {2021-02-18},
	journal = {SSRN Electronic Journal},
	author = {Gaynor, Martin and Ginsburg, Paul B.},
	year = {2017},
	file = {Gaynor and Ginsburg - 2017 - Making Health Care Markets Work Competition Polic.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\ZMJ6F5PE\\Gaynor and Ginsburg - 2017 - Making Health Care Markets Work Competition Polic.pdf:application/pdf},
}

@article{griffith_diminishing_2018,
	title = {Diminishing {Insurance} {Choices} {In} {The} {Affordable} {Care} {Act} {Marketplaces}: {A} {County}-{Based} {Analysis}},
	volume = {37},
	issn = {0278-2715, 1544-5208},
	shorttitle = {Diminishing {Insurance} {Choices} {In} {The} {Affordable} {Care} {Act} {Marketplaces}},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2018.0701},
	doi = {10.1377/hlthaff.2018.0701},
	abstract = {While the Affordable Care Act has expanded health insurance to millions of Americans through the expansion of eligibility for Medicaid and the health insurance Marketplaces, concerns about Marketplace stability persist—given increasing premiums and multiple insurers exiting selected markets. Yet there has been little investigation of what factors underlie this pattern. We assessed the county-level prevalence of limited insurer participation (defined as having two or fewer distinct participating insurers) in Marketplaces in the period 2014–18. Overall, in 2015 and 2016 rates of insurer participation were largely stable, and approximately 80 percent of counties (containing 93 percent of US residents) had at least three Marketplace insurers. However, these proportions declined sharply starting in 2017, falling to 36 percent of counties and 60 percent of the population in 2018. We also examined county-level factors associated with limited insurer competition and found that it occurred disproportionately in rural counties, those with higher mortality rates, and those where insurers had lower medical loss ratios (that is, potentially higher profit margins), as well as in states where Republicans controlled the executive and legislative branches of government. Decreased competition was less common in states with higher proportions of residents who were Hispanic or ages 45–64 and states that chose to expand Medicaid.},
	language = {en},
	number = {10},
	urldate = {2021-02-18},
	journal = {Health Affairs},
	author = {Griffith, Kevin and Jones, David K. and Sommers, Benjamin D.},
	month = oct,
	year = {2018},
	pages = {1678--1684},
	file = {Griffith et al. - 2018 - Diminishing Insurance Choices In The Affordable Ca.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\25E3NGCI\\Griffith et al. - 2018 - Diminishing Insurance Choices In The Affordable Ca.pdf:application/pdf},
}

@article{nan_feature-budgeted_nodate,
	title = {Feature-{Budgeted} {Random} {Forest}},
	abstract = {We seek decision rules for prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-speciﬁed average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which ampliﬁes costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax costweighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate competitive accuracy-cost curves against state-of-the-art prediction-time algorithms.},
	language = {en},
	author = {Nan, Feng and Wang, Joseph and Saligrama, Venkatesh},
	pages = {9},
	file = {Nan et al. - Feature-Budgeted Random Forest.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\PIZGXR5Z\\Nan et al. - Feature-Budgeted Random Forest.pdf:application/pdf},
}

@misc{cerulli_sctree_2020,
	title = {{SCTREE}: {Stata} module to implement classification trees via optimal pruning, bagging, random forests, and boosting methods},
	shorttitle = {{SCTREE}},
	url = {https://ideas.repec.org/c/boc/bocode/s458645.html},
	abstract = {sctree is a Stata wrapper for the R functions "tree()", "randomForest()", and "gbm()". It allows to implement the following classification tree models: (1) classification tree with optimal pruning, (2) bagging, (3) random forests, and (4) boosting.},
	urldate = {2021-02-18},
	publisher = {Boston College Department of Economics},
	author = {Cerulli, Giovanni},
	month = may,
	year = {2020},
	note = {Language: en
Publication Title: Statistical Software Components},
	keywords = {bagging, boosting, classification trees, pruning, random forests, Stata},
	file = {Snapshot:C\:\\Users\\jtg3519\\Zotero\\storage\\ENVZGS7T\\s458645.html:text/html},
}

@misc{bivand_rgeos_2020,
	title = {rgeos: {Interface} to {Geometry} {Engine} - {Open} {Source} ('{GEOS}')},
	copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL (≥ 2)]},
	shorttitle = {rgeos},
	url = {https://CRAN.R-project.org/package=rgeos},
	abstract = {Interface to Geometry Engine - Open Source ('GEOS') using the C 'API' for topology operations on geometries. The 'GEOS' library is external to the package, and, when installing the package from source, must be correctly installed first. Windows and Mac Intel OS X binaries are provided on 'CRAN'. ('rgeos' {\textgreater}= 0.5-1): Up to and including 'GEOS' 3.7.1, topological operations succeeded with some invalid geometries for which the same operations fail from and including 'GEOS' 3.7.2. The 'checkValidity=' argument defaults and structure have been changed, from default FALSE to integer default '0L' for 'GEOS' {\textless} 3.7.2 (no check), '1L' 'GEOS' {\textgreater}= 3.7.2 (check and warn). A value of '2L' is also provided that may be used, assigned globally using 'set\_RGEOS\_CheckValidity(2L)', or locally using the 'checkValidity=2L' argument, to attempt zero-width buffer repair if invalid geometries are found. The previous default (FALSE, now '0L') is fastest and used for 'GEOS' {\textless} 3.7.2, but will not warn users of possible problems before the failure of topological operations that previously succeeded. From 'GEOS' 3.8.0, repair of geometries may also be attempted using 'gMakeValid()', which may, however, return a collection of geometries of different types.},
	urldate = {2021-02-18},
	author = {Bivand, Roger and Rundel, Colin and Pebesma, Edzer and Stuetz, Rainer and Hufthammer, Karl Ove and Giraudoux, Patrick and Davis, Martin and Santilli, Sandro},
	month = sep,
	year = {2020},
	keywords = {Spatial},
}

@article{kahle_ggmap_2013,
	title = {ggmap: {Spatial} {Visualization} with ggplot2},
	volume = {5},
	issn = {2073-4859},
	shorttitle = {ggmap},
	url = {https://journal.r-project.org/archive/2013/RJ-2013-014/index.html},
	doi = {10.32614/RJ-2013-014},
	abstract = {In spatial statistics the ability to visualize data and models superimposed with their basic social landmarks and geographic context is invaluable. ggmap is a new tool which enables such visualization by combining the spatial information of static maps from Google Maps, OpenStreetMap, Stamen Maps or CloudMade Maps with the layered grammar of graphics implementation of ggplot2. In addition, several new utility functions are introduced which allow the user to access the Google Geocoding, Distance Matrix, and Directions APIs. The result is an easy, consistent and modular framework for spatial graphics with several convenient tools for spatial data analysis.},
	language = {en},
	number = {1},
	urldate = {2021-02-18},
	journal = {The R Journal},
	author = {Kahle, David and Wickham, Hadley},
	year = {2013},
	pages = {144},
	file = {Kahle and Wickham - 2013 - ggmap Spatial Visualization with ggplot2.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\FMTX5AQ3\\Kahle and Wickham - 2013 - ggmap Spatial Visualization with ggplot2.pdf:application/pdf},
}

@article{strobl_bias_2007,
	title = {Bias in random forest variable importance measures: {Illustrations}, sources and a solution},
	volume = {8},
	issn = {1471-2105},
	shorttitle = {Bias in random forest variable importance measures},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25},
	doi = {10.1186/1471-2105-8-25},
	abstract = {Background: Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.
Results: Simulation studies are presented illustrating that, when random forest variable importance measures are used with data of varying types, the results are misleading because suboptimal predictor variables may be artificially preferred in variable selection. The two mechanisms underlying this deficiency are biased variable selection in the individual classification trees used to build the random forest on one hand, and effects induced by bootstrap sampling with replacement on the other hand.
Conclusion: We propose to employ an alternative implementation of random forests, that provides unbiased variable selection in the individual classification trees. When this method is applied using subsampling without replacement, the resulting variable importance measures can be used reliably for variable selection even in situations where the potential predictor variables vary in their scale of measurement or their number of categories. The usage of both random forest algorithms and their variable importance measures in the R system for statistical computing is illustrated and documented thoroughly in an application re-analyzing data from a study on RNA editing. Therefore the suggested method can be applied straightforwardly by scientists in bioinformatics research.},
	language = {en},
	number = {1},
	urldate = {2021-02-18},
	journal = {BMC Bioinformatics},
	author = {Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten},
	month = dec,
	year = {2007},
	pages = {25},
	file = {Strobl et al. - 2007 - Bias in random forest variable importance measures.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\PJ4Q2ET3\\Strobl et al. - 2007 - Bias in random forest variable importance measures.pdf:application/pdf},
}

@article{schonlau_random_2020,
	title = {The random forest algorithm for statistical learning},
	volume = {20},
	issn = {1536-867X, 1536-8734},
	url = {http://journals.sagepub.com/doi/10.1177/1536867X20909688},
	doi = {10.1177/1536867X20909688},
	abstract = {Random forests (Breiman, 2001, Machine Learning 45: 5–32) is a statistical- or machine-learning algorithm for prediction. In this article, we introduce a corresponding new command, rforest. We overview the random forest algorithm and illustrate its use with two examples: The ﬁrst example is a classiﬁcation problem that predicts whether a credit card holder will default on his or her debt. The second example is a regression problem that predicts the logscaled number of shares of online news articles. We conclude with a discussion that summarizes key points demonstrated in the examples.},
	language = {en},
	number = {1},
	urldate = {2021-02-18},
	journal = {The Stata Journal: Promoting communications on statistics and Stata},
	author = {Schonlau, Matthias and Zou, Rosie Yuyan},
	month = mar,
	year = {2020},
	pages = {3--29},
	file = {Schonlau and Zou - 2020 - The random forest algorithm for statistical learni.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\ZU8E9RTW\\Schonlau and Zou - 2020 - The random forest algorithm for statistical learni.pdf:application/pdf},
}

@article{lindrooth_understanding_2018,
	title = {Understanding {The} {Relationship} {Between} {Medicaid} {Expansions} {And} {Hospital} {Closures}},
	volume = {37},
	issn = {0278-2715, 1544-5208},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2017.0976},
	doi = {10.1377/hlthaff.2017.0976},
	abstract = {Decisions by states about whether to expand Medicaid under the Affordable Care Act (ACA) have implications for hospitals’ financial health. We hypothesized that Medicaid expansion of eligibility for childless adults prevents hospital closures because increased Medicaid coverage for previously uninsured people reduces uncompensated care expenditures and strengthens hospitals’ financial position. We tested this hypothesis using data for the period 2008–16 on hospital closures and financial performance. We found that the ACA’s Medicaid expansion was associated with improved hospital financial performance and substantially lower likelihoods of closure, especially in rural markets and counties with large numbers of uninsured adults before Medicaid expansion. Future congressional efforts to reform Medicaid policy should consider the strong relationship between Medicaid coverage levels and the financial viability of hospitals. Our results imply that reverting to pre-ACA eligibility levels would lead to particularly large increases in rural hospital closures. Such closures could lead to reduced access to care and a loss of highly skilled jobs, which could have detrimental impacts on local economies.},
	language = {en},
	number = {1},
	urldate = {2021-02-18},
	journal = {Health Affairs},
	author = {Lindrooth, Richard C. and Perraillon, Marcelo C. and Hardy, Rose Y. and Tung, Gregory J.},
	month = jan,
	year = {2018},
	pages = {111--120},
	file = {Lindrooth et al. - 2018 - Understanding The Relationship Between Medicaid Ex.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\535LMLR6\\Lindrooth et al. - 2018 - Understanding The Relationship Between Medicaid Ex.pdf:application/pdf},
}

@article{das_cost_2020,
	title = {Cost {Aware} {Feature} {Elicitation}},
	abstract = {Motivated by clinical tasks where acquiring certain features such as FMRI or blood tests can be expensive, we address the problem of test-time elicitation of features. We formulate the problem of costaware feature elicitation as an optimization problem with trade-off between performance and feature acquisition cost. Our experiments on three real-world medical tasks demonstrate the efficacy and effectiveness of our proposed approach in minimizing costs and maximizing performance.},
	language = {en},
	journal = {San Diego},
	author = {Das, Srijita and Iyer, Rishabh and Natarajan, Sriraam},
	year = {2020},
	pages = {6},
	file = {Das et al. - 2020 - Cost Aware Feature Elicitation.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\DKRU4GYE\\Das et al. - 2020 - Cost Aware Feature Elicitation.pdf:application/pdf},
}

@misc{parr_beware_nodate,
	title = {Beware {Default} {Random} {Forest} {Importances}},
	url = {http://explained.ai/decision-tree-viz/index.html},
	abstract = {Training a model that accurately predicts outcomes is great, but most of the time you don't just need predictions, you want to be able to interpret your model. The problem is that the scikit-learn Random Forest feature importance and R's default Random Forest feature importance strategies are biased. To get reliable results in Python, use permutation importance, provided here and in our rfpimp package (via pip). For R, use importance=T in the Random Forest constructor then type=1 in R's importance() function.},
	urldate = {2021-02-19},
	author = {Parr, Terrence and Turgutlu, Kerem},
	file = {Parr and Turgutlu - Beware Default Random Forest Importances.html:C\:\\Users\\jtg3519\\Zotero\\storage\\8DS3AZW2\\Parr and Turgutlu - Beware Default Random Forest Importances.html:text/html},
}

@article{li_feature_2018,
	title = {Feature {Selection}: {A} {Data} {Perspective}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Feature {Selection}},
	url = {https://dl.acm.org/doi/10.1145/3136625},
	doi = {10.1145/3136625},
	language = {en},
	number = {6},
	urldate = {2021-02-19},
	journal = {ACM Computing Surveys},
	author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
	month = jan,
	year = {2018},
	pages = {1--45},
	file = {Li et al. - 2018 - Feature Selection A Data Perspective.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\4T5NB79P\\Li et al. - 2018 - Feature Selection A Data Perspective.pdf:application/pdf},
}

@article{janitza_computationally_2018,
	title = {A computationally fast variable importance test for random forests for high-dimensional data},
	volume = {12},
	issn = {1862-5347, 1862-5355},
	url = {http://link.springer.com/10.1007/s11634-016-0276-4},
	doi = {10.1007/s11634-016-0276-4},
	abstract = {Random forests are a commonly used tool for classiﬁcation and for ranking candidate predictors based on the so-called variable importance measures. These measures attribute scores to the variables reﬂecting their importance. A drawback of variable importance measures is that there is no natural cutoff that can be used to discriminate between important and non-important variables. Several approaches, for example approaches based on hypothesis testing, were developed for addressing this problem. The existing testing approaches require the repeated computation of random forests. While for low-dimensional settings those approaches might be computationally tractable, for high-dimensional settings typically including thousands of candidate predictors, computing time is enormous. In this article a computationally fast heuristic variable importance test is proposed that is appropriate for high-dimensional data where many variables do not carry any information. The testing approach is based on a modiﬁed version of the permutation variable importance, which is inspired by crossvalidation procedures. The new approach is tested and compared to the approach of Altmann and colleagues using simulation studies, which are based on real data from high-dimensional binary classiﬁcation settings. The new approach controls the type I error and has at least comparable power at a substantially smaller computation time in the studies. Thus, it might be used as a computationally fast alternative to existing procedures for high-dimensional data settings where many variables do not carry any information. The new approach is implemented in the R package vita.},
	language = {en},
	number = {4},
	urldate = {2021-02-20},
	journal = {Advances in Data Analysis and Classification},
	author = {Janitza, Silke and Celik, Ender and Boulesteix, Anne-Laure},
	month = dec,
	year = {2018},
	pages = {885--915},
	file = {Janitza2018_Article_AComputationallyFastVariableIm.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\GMX5RY9I\\Janitza2018_Article_AComputationallyFastVariableIm.pdf:application/pdf},
}

@article{hapfelmeier_new_2013,
	title = {A new variable selection approach using {Random} {Forests}},
	volume = {60},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947312003490},
	doi = {10.1016/j.csda.2012.09.020},
	abstract = {Random Forests are frequently applied as they achieve a high prediction accuracy and have the ability to identify informative variables. Several approaches for variable selection have been proposed to combine and intensify these qualities. An extensive review of the corresponding literature led to the development of a new approach that is based on the theoretical framework of permutation tests and meets important statistical properties. A comparison to another eight popular variable selection methods in three simulation studies and four real data applications indicated that: the new approach can also be used to control the test-wise and family-wise error rate, provides a higher power to distinguish relevant from irrelevant variables and leads to models which are located among the very best performing ones. In addition, it is equally applicable to regression and classification problems.},
	language = {en},
	urldate = {2021-02-20},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hapfelmeier, A. and Ulm, K.},
	month = apr,
	year = {2013},
	pages = {50--69},
	file = {Hapfelmeier and Ulm - 2013 - A new variable selection approach using Random For.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\BEE375YU\\Hapfelmeier and Ulm - 2013 - A new variable selection approach using Random For.pdf:application/pdf},
}

@article{altmann_permutation_2010,
	title = {Permutation importance: a corrected feature importance measure},
	volume = {26},
	issn = {1460-2059, 1367-4803},
	shorttitle = {Permutation importance},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btq134},
	doi = {10.1093/bioinformatics/btq134},
	abstract = {Motivation: In life sciences, interpretability of machine learning models is as important as their prediction accuracy. Linear models are probably the most frequently used methods for assessing feature relevance, despite their relative inﬂexibility. However, in the past years effective estimators of feature relevance have been derived for highly complex or non-parametric models such as support vector machines and RandomForest (RF) models. Recently, it has been observed that RF models are biased in such a way that categorical variables with a large number of categories are preferred.},
	language = {en},
	number = {10},
	urldate = {2021-02-20},
	journal = {Bioinformatics},
	author = {Altmann, André and Toloşi, Laura and Sander, Oliver and Lengauer, Thomas},
	month = may,
	year = {2010},
	pages = {1340--1347},
	file = {Altmann et al. - 2010 - Permutation importance a corrected feature import.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\3V78FL9R\\Altmann et al. - 2010 - Permutation importance a corrected feature import.pdf:application/pdf},
}

@article{goerigk_predicting_2020,
	title = {Predicting instructed simulation and dissimulation when screening for depressive symptoms},
	volume = {270},
	issn = {0940-1334, 1433-8491},
	url = {http://link.springer.com/10.1007/s00406-018-0967-2},
	doi = {10.1007/s00406-018-0967-2},
	abstract = {The intentional distortion of test results presents a fundamental problem to self-report-based psychiatric assessment, such as screening for depressive symptoms. The first objective of the study was to clarify whether depressed patients like healthy controls possess both the cognitive ability and motivation to deliberately influence results of commonly used screening measures. The second objective was the construction of a method derived directly from within the test takers’ responses to systematically detect faking behavior. Supervised machine learning algorithms posit the potential to empirically learn the implicit interconnections between responses, which shape detectable faking patterns. In a standardized design, faking bad and faking good were experimentally induced in a matched sample of 150 depressed and 150 healthy subjects. Participants completed commonly used questionnaires to detect depressive and associated symptoms. Group differences throughout experimental conditions were evaluated using linear mixed-models. Machine learning algorithms were trained on the test results and compared regarding their capacity to systematically predict distortions in response behavior in two scenarios: (1) differentiation of authentic patient responses from simulated responses of healthy participants; (2) differentiation of authentic patient responses from dissimulated patient responses. Statistically significant convergence of the test scores in both faking conditions suggests that both depressive patients and healthy controls have the cognitive ability as well as the motivational compliance to alter their test results. Evaluation of the algorithmic capability to detect faking behavior yielded ideal predictive accuracies of up to 89\%. Implications of the findings, as well as future research objectives are discussed. Trial Registration The study was pre-registered at the German registry for clinical trials (Deutsches Register klinischer Studien, DRKS; DRKS00007708).},
	language = {en},
	number = {2},
	urldate = {2021-02-22},
	journal = {European Archives of Psychiatry and Clinical Neuroscience},
	author = {Goerigk, Stephan and Hilbert, Sven and Jobst, Andrea and Falkai, Peter and Bühner, Markus and Stachl, Clemens and Bischl, Bernd and Coors, Stefan and Ehring, Thomas and Padberg, Frank and Sarubin, Nina},
	month = mar,
	year = {2020},
	pages = {153--168},
	file = {Goerigk et al. - 2020 - Predicting instructed simulation and dissimulation.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\IADJPCI7\\Goerigk et al. - 2020 - Predicting instructed simulation and dissimulation.pdf:application/pdf},
}

@article{nuchel_recent_2017,
	title = {Recent tree cover increases in eastern {China} linked to low, declining human pressure, steep topography, and climatic conditions favoring tree growth},
	volume = {12},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0177552},
	doi = {10.1371/journal.pone.0177552},
	language = {en},
	number = {6},
	urldate = {2021-02-22},
	journal = {PLOS ONE},
	author = {Nüchel, Jonas and Svenning, Jens-Christian},
	editor = {Zang, RunGuo},
	month = jun,
	year = {2017},
	pages = {e0177552},
	file = {Nüchel and Svenning - 2017 - Recent tree cover increases in eastern China linke.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\6C7HS3XH\\Nüchel and Svenning - 2017 - Recent tree cover increases in eastern China linke.pdf:application/pdf},
}

@article{speiser_comparison_2019,
	title = {A comparison of random forest variable selection methods for classification prediction modeling},
	volume = {134},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417419303574},
	doi = {10.1016/j.eswa.2019.05.028},
	abstract = {Random forest classification is a popular machine learning method for developing prediction models in many research settings. Often in prediction modeling, a goal is to reduce the number of variables needed to obtain a prediction in order to reduce the burden of data collection and improve efficiency. Several variable selection methods exist for the setting of random forest classification; however, there is a paucity of literature to guide users as to which method may be preferable for different types of datasets. Using 311 classification datasets freely available online, we evaluate the prediction error rates, number of variables, computation times and area under the receiver operating curve for many random forest variable selection methods. We compare random forest variable selection methods for different types of datasets (datasets with binary outcomes, datasets with many predictors, and datasets with imbalanced outcomes) and for different types of methods (standard random forest versus conditional random forest methods and test based versus performance based methods). Based on our study, the best variable selection methods for most datasets are Jiang's method and the method implemented in the VSURF R package. For datasets with many predictors, the methods implemented in the R packages varSelRF and Boruta are preferable due to computational efficiency. A significant contribution of this study is the ability to assess different variable selection techniques in the setting of random forest classification in order to identify preferable methods based on applications in expert and intelligent systems.},
	language = {en},
	urldate = {2021-02-22},
	journal = {Expert Systems with Applications},
	author = {Speiser, Jaime Lynn and Miller, Michael E. and Tooze, Janet and Ip, Edward},
	month = nov,
	year = {2019},
	keywords = {Classification, Feature reduction, Random forest, Variable selection},
	pages = {93--101},
	file = {Speiser et al. - 2019 - A comparison of random forest variable selection m.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\3Z2X85L6\\Speiser et al. - 2019 - A comparison of random forest variable selection m.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jtg3519\\Zotero\\storage\\PLGFUPDZ\\S0957417419303574.html:text/html},
}

@article{genuer_vsurf_2015,
	title = {{VSURF}: {An} {R} {Package} for {Variable} {Selection} {Using} {Random} {Forests}},
	volume = {7},
	issn = {2073-4859},
	shorttitle = {{VSURF}},
	url = {https://journal.r-project.org/archive/2015/RJ-2015-018/index.html},
	doi = {10.32614/RJ-2015-018},
	abstract = {This paper describes the R package VSURF. Based on random forests, and for both regression and classiﬁcation problems, it returns two subsets of variables. The ﬁrst is a subset of important variables including some redundancy which can be relevant for interpretation, and the second one is a smaller subset corresponding to a model trying to avoid redundancy focusing more closely on the prediction objective. The two-stage strategy is based on a preliminary ranking of the explanatory variables using the random forests permutation-based score of importance and proceeds using a stepwise forward strategy for variable introduction. The two proposals can be obtained automatically using data-driven default values, good enough to provide interesting results, but strategy can also be tuned by the user. The algorithm is illustrated on a simulated example and its applications to real datasets are presented.},
	language = {en},
	number = {2},
	urldate = {2021-02-22},
	journal = {The R Journal},
	author = {Genuer, Robin and Poggi, Jean-Michel and Tuleau-Malot, Christine},
	year = {2015},
	pages = {19},
	file = {Genuer et al. - 2015 - VSURF An R Package for Variable Selection Using R.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\6LF9U9V6\\Genuer et al. - 2015 - VSURF An R Package for Variable Selection Using R.pdf:application/pdf},
}

@article{berkowitz_association_2019,
	title = {Association between access to social service resources and cardiometabolic risk factors: a machine learning and multilevel modeling analysis},
	volume = {9},
	issn = {2044-6055, 2044-6055},
	shorttitle = {Association between access to social service resources and cardiometabolic risk factors},
	url = {https://bmjopen.bmj.com/lookup/doi/10.1136/bmjopen-2018-025281},
	doi = {10.1136/bmjopen-2018-025281},
	abstract = {Objectives  Interest in linking patients with unmet social needs to area-level resources, such as food pantries and employment centres in one’s ZIP code, is growing. However, whether the presence of these resources is associated with better health outcomes is unclear. We sought to determine if area-level resources, defined as organisations that assist individuals with meeting healthrelated social needs, are associated with lower levels of cardiometabolic risk factors. Design Cross-sectional. Setting  Data were collected in a primary care network in eastern Massachusetts in 2015. Participants and primary and secondary outcome measures  123 355 participants were included. The primary outcome was body mass index (BMI). The secondary outcomes were systolic blood pressure (SBP), low-density lipoprotein (LDL) cholesterol and haemoglobin A1c (HbA1c). All participants were included in BMI analyses. Participants with hypertension were included in SBP analyses. Participants with an indication for cholesterol lowering were included in LDL analyses and participants with diabetes mellitus were included in HbA1c analyses. We used a random forest-based machine-learning algorithm to identify types of resources associated with study outcomes. We then tested the association of ZIP-level selected resource types (three for BMI, two each for SBP and HbA1c analyses and one for LDL analyses) with these outcomes, using multilevel models to account for individual-level, clinic-level and other area-level factors.
Results  Resources associated with lower BMI included more food resources (−0.08 kg/m2 per additional resource, 95\% CI −0.13 to −0.03 kg/m2), employment resources (−0.05 kg/m2, 95\% CI −0.11 to −0.002 kg/m2) and nutrition resources (−0.07 kg/m2, 95\% CI −0.13 to −0.01 kg/m2). No area resources were associated with differences in SBP, LDL or HbA1c.
Conclusions  Access to specific local resources is associated with better BMI. Efforts to link patients to area resources, and to improve the resources landscape within communities, may help reduce BMI and improve population health.},
	language = {en},
	number = {3},
	urldate = {2021-02-22},
	journal = {BMJ Open},
	author = {Berkowitz, Seth A and Basu, Sanjay and Venkataramani, Atheendar and Reznor, Gally and Fleegler, Eric W and Atlas, Steven J},
	month = mar,
	year = {2019},
	pages = {e025281},
	file = {Berkowitz et al. - 2019 - Association between access to social service resou.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\CSVQ3Z78\\Berkowitz et al. - 2019 - Association between access to social service resou.pdf:application/pdf},
}

@article{genuer_variable_2010,
	title = {Variable selection using random forests},
	volume = {31},
	issn = {01678655},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865510000954},
	doi = {10.1016/j.patrec.2010.03.014},
	abstract = {This paper proposes, focusing on random forests, the increasingly used statistical method for classiﬁcation and regression problems introduced by Leo Breiman in 2001, to investigate two classical issues of variable selection. The ﬁrst one is to ﬁnd important variables for interpretation and the second one is more restrictive and try to design a good parsimonious prediction model. The main contribution is twofold: to provide some experimental insights about the behavior of the variable importance index based on random forests and to propose a strategy involving a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy.},
	language = {en},
	number = {14},
	urldate = {2021-02-22},
	journal = {Pattern Recognition Letters},
	author = {Genuer, Robin and Poggi, Jean-Michel and Tuleau-Malot, Christine},
	month = oct,
	year = {2010},
	pages = {2225--2236},
	file = {Genuer et al. - 2010 - Variable selection using random forests.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\HH4EQ9FX\\Genuer et al. - 2010 - Variable selection using random forests.pdf:application/pdf},
}

@article{shapiro_how_nodate,
	title = {How to {Give} an {Applied} {Micro} {Talk}},
	language = {en},
	author = {Shapiro, Jesse M},
	pages = {43},
	file = {Shapiro - How to Give an Applied Micro Talk.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\I6V53AFR\\Shapiro - How to Give an Applied Micro Talk.pdf:application/pdf},
}

@article{fox_bootstrapping_2017,
	title = {Bootstrapping {Regression} {Models} in {R}},
	abstract = {The bootstrap is a general approach to statistical inference based on building a sampling distribution for a statistic by resampling from the data at hand. This appendix to Fox and Weisberg (2011) brieﬂy describes the rationale for the bootstrap and explains how to bootstrap regression models using the Boot function, which was added to the car package in 2012, and therefore is not described in Fox and Weisberg (2011). This function provides a simple way to access the power of the boot function (lower-case “b”) in the boot package.},
	language = {en},
	journal = {An R Companion to Applied Regression, Second Edition},
	author = {Fox, John and Weisberg, Sanford},
	month = oct,
	year = {2017},
	pages = {20},
	file = {Fox and Weisberg - Bootstrapping Regression Models in R.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\LXF8Y8L3\\Fox and Weisberg - Bootstrapping Regression Models in R.pdf:application/pdf},
}

@article{blavin_medicaid_2021,
	title = {Medicaid {Expansion}: {Effects} {On} {Hospital} {Finances} {And} {Implications} {For} {Hospitals} {Facing} {COVID}-19 {Challenges}: {Study} examines {Medicaid} expansion effects on hospital finances and implications for hospitals facing {COVID}-19 challenges.},
	volume = {40},
	issn = {0278-2715, 1544-5208},
	shorttitle = {Medicaid {Expansion}},
	url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.2020.00502},
	doi = {10.1377/hlthaff.2020.00502},
	abstract = {States’ decisions to expand Medicaid may have important implications for their hospitals’ financial ability to weather the coronavirus disease 2019 (COVID-19) pandemic. This study estimated the effects of the Affordable Care Act (ACA) Medicaid expansion on hospital finances in 2017 to update earlier findings. The analysis also explored how the ACA Medicaid expansion affects different types of hospitals by size, ownership, rurality, and safety-net status. We found that the early positive financial impact of Medicaid expansion was sustained in fiscal years 2016 and 2017 as hospitals in expansion states continued to experience decreased uncompensated care costs and increased Medicaid revenue and financial margins. The magnitude of these impacts varied by hospital type. As COVID-19 has brought hospitals to a time of great need, findings from this study provide important information on what hospitals in states that have yet to expand Medicaid could gain through expansion and what is at risk should any reversal of Medicaid expansions occur.},
	language = {en},
	number = {1},
	urldate = {2021-02-24},
	journal = {Health Affairs},
	author = {Blavin, Fredric and Ramos, Christal},
	month = jan,
	year = {2021},
	pages = {82--90},
	file = {Blavin and Ramos - 2021 - Medicaid Expansion Effects On Hospital Finances A.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\2WHK2J4P\\Blavin and Ramos - 2021 - Medicaid Expansion Effects On Hospital Finances A.pdf:application/pdf},
}

@article{rajkumar_marylands_2014,
	title = {Maryland's {All}-{Payer} {Approach} to {Delivery}-{System} {Reform}},
	volume = {370},
	copyright = {Copyright © 2014 Massachusetts Medical Society. All rights reserved.},
	issn = {0028-4793},
	abstract = {Maryland, with its all-payer rate-setting system for hospital services, and the Centers for Medicare and Medicaid Services are launching a new model that will transform the state's delivery system and may guide other federal–state partnerships in improving health care. On January 10, 2014, the Centers for Medicare and Medicaid Services (CMS) and the State of Maryland jointly announced the launch of a statewide model that will transform Maryland's health care delivery system. Although some aspects of the new approach may be unique to Maryland and not applicable elsewhere, both the principles of this model and the process that led to its development may serve as a guide for future federal–state partnership efforts aiming to improve health care and to lower costs through an all-payer approach. Since the late 1970s, Maryland has operated what is now the country's only all-payer . . .},
	language = {eng},
	number = {6},
	journal = {The New England Journal of Medicine},
	author = {Rajkumar, Rahul and Patel, Ankit and Murphy, Karen and Colmers, John M and Blum, Jonathan D and Conway, Patrick H and Sharfstein, Joshua M},
	year = {2014},
	note = {Place: United States
Publisher: Massachusetts Medical Society},
	keywords = {Abridged Index Medicus, Advisory Committees, Centers for Medicare and Medicaid Services (U.S.), Cost Allocation, Cost shifting, Economics, Growth rate, Health - organization \& administration, Health care, Health care delivery, Hospital, Hospital Administration, Index Medicus, Insurance, Maryland, Medicare, Medicare - economics, Medicare - legislation \& jurisprudence, Patient Readmission - statistics \& numerical data, Per capita, Rate Setting and Review, State Government, United States},
	pages = {493--495},
}

@article{fiedler_capping_2020,
	title = {Capping {Prices} or {Creating} a {Public} {Option}: {How} {Would} {They} {Change} {What} {We} {Pay} for {Health} {Care}?},
	language = {en},
	journal = {The Brookings Institution},
	author = {Fiedler, Matthew},
	month = nov,
	year = {2020},
	pages = {154},
	file = {Fiedler - Capping Prices or Creating a Public Option How Wo.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\MJP4E3XJ\\Fiedler - Capping Prices or Creating a Public Option How Wo.pdf:application/pdf},
}

@article{murray_hospital_2015,
	title = {Hospital {Rate} {Setting} {Revisited}},
	abstract = {Background 1 Reconsideration of All-Payer Rate Systems 5
Purpose of the Report and Method 6 Chapter 2. A Primer on State-Based Rate Setting Systems 7 Impetus and Context for Development 7 Rate Setting Defined 9 Basic Design Elements of Hospital-Based Rate Setting 11 The Importance of Prospective Systems 11 The Base Rate and Adjustments 13 The Method of Updating Rates 14 The Structure of Payments and Constraints 15 Rate Adjustments 18 Compliance 20 The Rise and Fall of Rate Setting in the United States 21 State Rate Setting Experiences—A Summary 22 Aftermath 29
Conclusion},
	language = {en},
	journal = {The Urban Institute},
	author = {Murray, Robert and Berenson, Robert A},
	month = nov,
	year = {2015},
	pages = {109},
	file = {Murray and Berenson - Hospital Rate Setting Revisited.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\INTZNSVC\\Murray and Berenson - Hospital Rate Setting Revisited.pdf:application/pdf},
}

@article{scheffler_consolidation_2018,
	title = {Consolidation {Trends} {In} {California}’s {Health} {Care} {System}: {Impacts} {On} {ACA} {Premiums} {And} {Outpatient} {Visit} {Prices}},
	volume = {37},
	url = {https://doi.org/10.1377/hlthaff.2018.0472},
	doi = {10.1377/hlthaff.2018.0472},
	abstract = {California has heavily concentrated hospital, physician, and health insurance markets, but their current structure and functioning is not well understood. We assessed consolidation trends and performed an analysis of “hot spots”—markets that potentially warrant concern and scrutiny by regulators in terms of both horizontal concentration (such as hospital-hospital mergers) and vertical integration (hospitals’ acquisition of physician practices). In 2016, seven counties were high on all six measures used in our hot-spot analysis (four horizontal concentration and two vertical integration measures), and five counties were high on five. The percentage of physicians in practices owned by a hospital increased from about 25 percent in 2010 to more than 40 percent in 2016. The estimated impact of the increase in vertical integration from 2013 to 2016 in highly concentrated hospital markets was found to be associated with a 12 percent increase in Marketplace premiums. For physician outpatient services, the increase in vertical integration was also associated with a 9 percent increase in specialist prices and a 5 percent increase in primary care prices. Legislative proposals, actions by the state’s attorney general, and other regulatory changes are suggested.},
	number = {9},
	journal = {Health Affairs},
	author = {Scheffler, Richard M. and Arnold, Daniel R. and Whaley, Christopher M.},
	year = {2018},
	pmid = {30179552},
	note = {\_eprint: https://doi.org/10.1377/hlthaff.2018.0472},
	pages = {1409--1416},
	file = {Scheffler et al. - 2018 - Consolidation Trends In California’s Health Care S.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\HZGFH6H7\\Scheffler et al. - 2018 - Consolidation Trends In California’s Health Care S.pdf:application/pdf},
}

@article{haeder_narrow_2015,
	title = {Narrow {Networks} and the {Affordable} {Care} {Act}},
	volume = {314},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2015.6807},
	doi = {10.1001/jama.2015.6807},
	abstract = {Much has been written about the Affordable Care Act (ACA) in the 5 years since its inception, both in scholarly journals and the popular press. Initial interest focused on states’ decisions about whether to implement their own insurance marketplaces and accept Medicaid expansion. There were countless accounts of dismal enrollment experiences marred by technical glitches during the initial enrollment period in late 2013 and early 2014. With implementation issues settled by the Supreme Court, and state and federal websites much improved, attention has recently turned to the operational aspects of the insurance markets. In particular, concerns have been increasing about the often limited hospital and physician networks offered by insurance plans sold in the marketplaces.},
	number = {7},
	journal = {JAMA},
	author = {Haeder, Simon F. and Weimer, David L. and Mukamel, Dana B.},
	year = {2015},
	note = {\_eprint: https://jamanetwork.com/journals/jama/articlepdf/2367284/jvp150096.pdf},
	pages = {669--670},
}

@article{dafny_more_2015,
	title = {More {Insurers}, {Lower} {Premiums}: {Evidence} from {Initial} {Pricing} in the {Health} {Insurance} {Marketplaces}},
	volume = {1},
	issn = {2332-3493},
	doi = {10.1162/ajhe a 00003},
	abstract = {First-year insurer participation in the Health Insurance Marketplaces (HIMs) established by the Affordable Care Act is limited in many areas of the country. There are 3.9 participants, on (population-weighted) average, in the 395 ratings areas spanning the 34 states with federally facilitated marketplaces (FFMs). Using data on the plans offered in the FFMs, together with predicted market shares for HIM participants (estimated using 2011 insurer-state market shares in the individual insurance market), we study the impact of competition on premiums. We exploit variation in ratings-area-level competition induced by UnitedHealthcare’s decision not to participate in any of the FFMs. We estimate that the second-lowest-price silver premium (which is directly linked to federal subsidies) would have decreased by 5.4 percent, on average, had UnitedHealthcare participated. If all insurers active in each state’s individual insurance market in 2011 had participated in all ratings areas in that state’s HIM, we estimate this key premium would be 11.1\% lower and 2014 federal subsidies would be reduced by \$1.7 billion.},
	language = {English (US)},
	number = {1},
	journal = {American Journal of Health Economics},
	author = {Dafny, Leemore S and Gruber, Jonathan and Ody, Christopher J},
	year = {2015},
	note = {Publisher: MIT Press Journals},
	pages = {53--81},
	file = {Dafny et al. - More Insurers Lower Premiums Evidence from Initia.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\XA3DEPY9\\Dafny et al. - More Insurers Lower Premiums Evidence from Initia.pdf:application/pdf},
}

@article{dafny_narrow_2017,
	title = {Narrow {Networks} {On} {The} {Health} {Insurance} {Marketplaces}: {Prevalence}, {Pricing}, {And} {The} {Cost} {Of} {Network} {Breadth}},
	volume = {36},
	url = {https://doi.org/10.1377/hlthaff.2016.1669},
	doi = {10.1377/hlthaff.2016.1669},
	abstract = {Anecdotal reports and systematic research highlight the prevalence of narrow-network plans on the Affordable Care Act’s health insurance Marketplaces. At the same time, Marketplace premiums in the period 2014–16 were much lower than projected by the Congressional Budget Office in 2009. Using detailed data on the breadth of both hospital and physician networks, we studied the prevalence of narrow networks and quantified the association between network breadth and premiums. Controlling for many potentially confounding factors, we found that a plan with narrow physician and hospital networks was 16 percent cheaper than a plan with broad networks for both, and that narrowing the breadth of just one type of network was associated with a 6–9 percent decrease in premiums. Narrow-network plans also have a sizable impact on federal outlays, as they depress the premium of the second-lowest-price silver plan, to which subsidy amounts are linked. Holding all else constant, we estimate that federal subsidies would have been 10.8 percent higher in 2014 had Marketplaces required all plans to offer broad provider networks. Narrow networks are a promising source of potential savings for other segments of the commercial insurance market.},
	number = {9},
	journal = {Health Affairs},
	author = {Dafny, Leemore S. and Hendel, Igal and Marone, Victoria and Ody, Christopher},
	year = {2017},
	pmid = {28874488},
	note = {\_eprint: https://doi.org/10.1377/hlthaff.2016.1669},
	pages = {1606--1614},
	file = {Dafny et al. - 2017 - Narrow Networks On The Health Insurance Marketplac.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\VQYRRTDU\\Dafny et al. - 2017 - Narrow Networks On The Health Insurance Marketplac.pdf:application/pdf},
}

@article{cox_analysis_2015,
	title = {Analysis of 2016 {Premium} {Changes} and {Insurer} {Participation} in the {Affordable} {Care} {Act}’s {Health} {Insurance} {Marketplaces}},
	language = {en},
	author = {Cox, Cynthia and Claxton, Gary and Rosa, Ma and Levitt, Larry},
	month = jun,
	year = {2015},
	pages = {7},
	file = {Analysis of 2016 Premium Changes and Insurer Parti.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\UBGWK2I4\\Analysis of 2016 Premium Changes and Insurer Parti.pdf:application/pdf},
}

@article{strobl_conditional_2008,
	title = {Conditional variable importance for random forests},
	volume = {9},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-9-307},
	doi = {10.1186/1471-2105-9-307},
	abstract = {Background: Random forests are becoming increasingly popular in many scientific fields because they can cope with "small n large p" problems, complex interactions and even highly correlated predictor variables. Their variable importance measures have recently been suggested as screening tools for, e.g., gene expression studies. However, these variable importance measures show a bias towards correlated predictor variables.
Results: We identify two mechanisms responsible for this finding: (i) A preference for the selection of correlated predictors in the tree building process and (ii) an additional advantage for correlated predictor variables induced by the unconditional permutation scheme that is employed in the computation of the variable importance measure. Based on these considerations we develop a new, conditional permutation scheme for the computation of the variable importance measure.
Conclusion: The resulting conditional variable importance reflects the true impact of each predictor variable more reliably than the original marginal approach.},
	language = {en},
	number = {1},
	urldate = {2021-02-25},
	journal = {BMC Bioinformatics},
	author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
	month = dec,
	year = {2008},
	pages = {307},
	file = {Strobl et al. - 2008 - Conditional variable importance for random forests.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\C49ST5JI\\Strobl et al. - 2008 - Conditional variable importance for random forests.pdf:application/pdf},
}

@article{strobl_party_2009,
	title = {“{Party} on! – {A} {New}, {Conditional} {Variable} {Importance} {Measure} for {Random} {Forests} {Available} in the party {Package}.},
	volume = {1},
	url = {https://journal.r-project.org/archive/2009-2/RJournal_2009-2_Strobl~et~al.pdf},
	abstract = {Random forests are one of the most
popular statistical learning algorithms, and a
variety of methods for fitting random forests
and related recursive partitioning approaches is
available in R. This paper points out two important features of the random forest implementation cforest available in the party package: The
resulting forests are unbiased and thus preferable to the randomForest implementation available in randomForest if predictor variables are
of different types. Moreover, a conditional permutation importance measure has recently been
added to the party package, which can help evaluate the importance of correlated predictor variables. The rationale of this new measure is illustrated and hands-on advice is given for the usage
of recursive partitioning tools in R.},
	number = {2},
	urldate = {2021-02-25},
	journal = {The R Journal},
	author = {Strobl, Carolin and Hothorn, Torsten and Zeileis, Achim},
	year = {2009},
	pages = {14--17},
	file = {Strobl et al. - “Party on! – A New, Conditional Variable Importanc.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\HJM9VCRF\\Strobl et al. - “Party on! – A New, Conditional Variable Importanc.pdf:application/pdf},
}

@techreport{abadie_when_2017,
	address = {Cambridge, MA},
	title = {When {Should} {You} {Adjust} {Standard} {Errors} for {Clustering}?},
	url = {http://www.nber.org/papers/w24003.pdf},
	abstract = {In empirical work in economics it is common to report standard errors that account for clustering of units. Typically, the motivation given for the clustering adjustments is that unobserved components in outcomes for units within clusters are correlated. However, because correlation may occur across more than one dimension, this motivation makes it difficult to justify why researchers use clustering in some dimensions, such as geographic, but not others, such as age cohorts or gender. This motivation also makes it difficult to explain why one should not cluster with data from a randomized experiment. In this paper, we argue that clustering is in essence a design problem, either a sampling design or an experimental design issue. It is a sampling design issue if sampling follows a two stage process where in the first stage, a subset of clusters were sampled randomly from a population of clusters, and in the second stage, units were sampled randomly from the sampled clusters. In this case the clustering adjustment is justified by the fact that there are clusters in the population that we do not see in the sample. Clustering is an experimental design issue if the assignment is correlated within the clusters. We take the view that this second perspective best fits the typical setting in economics where clustering adjustments are used. This perspective allows us to shed new light on three questions: (i) when should one adjust the standard errors for clustering, (ii) when is the conventional adjustment for clustering appropriate, and (iii) when does the conventional adjustment of the standard errors matter.},
	language = {en},
	number = {w24003},
	urldate = {2021-03-13},
	institution = {National Bureau of Economic Research},
	author = {Abadie, Alberto and Athey, Susan and Imbens, Guido and Wooldridge, Jeffrey},
	month = nov,
	year = {2017},
	doi = {10.3386/w24003},
	pages = {w24003},
	file = {Abadie et al. - 2017 - When Should You Adjust Standard Errors for Cluster.pdf:C\:\\Users\\jtg3519\\Zotero\\storage\\K4MKKRYZ\\Abadie et al. - 2017 - When Should You Adjust Standard Errors for Cluster.pdf:application/pdf},
}
